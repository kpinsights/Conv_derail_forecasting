{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1012,"status":"ok","timestamp":1713386338740,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"},"user_tz":240},"id":"YvYPiIuktQMU","outputId":"efaacc35-1480-4a3d-896e-c912a4842d5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"14_13XMetkSb","executionInfo":{"status":"ok","timestamp":1713386341648,"user_tz":240,"elapsed":2911,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","\n","train_path = '/content/drive/MyDrive/Colab Notebooks/Conversation_derailment/conv_derailment_data/data/cga/base/train.json'\n","test_path = '/content/drive/MyDrive/Colab Notebooks/Conversation_derailment/conv_derailment_data/data/cga/base/test.json'\n","val_path = '/content/drive/MyDrive/Colab Notebooks/Conversation_derailment/conv_derailment_data/data/cga/base/valid.json'\n","\n","train_data = []\n","with open(train_path,'r') as f:\n","  for line in f:\n","    train_data.append(json.loads(line))\n","\n","test_data = []\n","with open(test_path,'r') as f:\n","  for line in f:\n","    test_data.append(json.loads(line))\n","\n","val_data = []\n","with open(val_path,'r') as f:\n","  for line in f:\n","    val_data.append(json.loads(line))"]},{"cell_type":"code","source":["from collections import defaultdict"],"metadata":{"id":"M5nzvT3zsbXp","executionInfo":{"status":"ok","timestamp":1713386341648,"user_tz":240,"elapsed":9,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["grouped_train = defaultdict(list)\n","for item in train_data:\n","    convo_id = item['convo_id']\n","    grouped_train[convo_id].append(item)\n","\n","\n","# duplicated convo_ids\n","duplicated_convo_ids = [k for k, v in grouped_train.items() if len(v) > 1]\n","print(duplicated_convo_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQhoF7ktS8uF","executionInfo":{"status":"ok","timestamp":1713386341649,"user_tz":240,"elapsed":9,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"6fb52b37-762e-4bd5-e365-5a4fe96a5df9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","source":["train_data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFrDf25DThsD","executionInfo":{"status":"ok","timestamp":1713386341649,"user_tz":240,"elapsed":6,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"f3a07718-cdca-4bbc-dde6-91df8433843b"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'src': ' i notice that earier that moved wiki _ link to bill chen citing wiki _ link, then you reverted this change, bill chen doesn\\'t commonly go by william, his book is even penned as bill chen. from what i read in wp : commonname patrikr seems to be correct, examples given are names such as : * wiki _ link ( not wiki _ link ) * wiki _ link ( not wiki _ link ) i think this revert may have been a mistake unless you know otherwise? ▪ [UNK] ▪ [SEP]  chen was known in the poker world as \" william \" for years before he became commonly known as \" bill \". i changed it back because incidences online including usenet are roughly equal, nothing at all like bill clinton and william clinton, and in equal cases using the real name seems the best choice. ( the external _ link page is especially pschizo... willam in the page title, bill in the page text ). however i suppose the book is the trump card, so using the name on the book is probably best.',\n"," 'reply': [101,\n","  1045,\n","  2156,\n","  2054,\n","  2017,\n","  3038,\n","  1045,\n","  2074,\n","  3191,\n","  2010,\n","  11662,\n","  14117,\n","  2015,\n","  6337,\n","  1010,\n","  2009,\n","  4930,\n","  2033,\n","  2043,\n","  1045,\n","  2387,\n","  1996,\n","  2689,\n","  2138,\n","  1045,\n","  3342,\n","  2032,\n","  2108,\n","  2170,\n","  3021,\n","  2043,\n","  1045,\n","  3427,\n","  1996,\n","  2197,\n","  2161,\n","  1997,\n","  2152,\n","  7533,\n","  11662,\n","  1010,\n","  2021,\n","  2017,\n","  4025,\n","  2000,\n","  2031,\n","  2116,\n","  2062,\n","  2086,\n","  3325,\n","  1999,\n","  1996,\n","  11662,\n","  1013,\n","  12219,\n","  2088,\n","  2059,\n","  1045,\n","  2079,\n","  1006,\n","  1045,\n","  1005,\n","  1049,\n","  2145,\n","  1037,\n","  2978,\n","  1997,\n","  1037,\n","  2047,\n","  11283,\n","  1007,\n","  1010,\n","  2061,\n","  1045,\n","  2359,\n","  2000,\n","  4638,\n","  2007,\n","  2017,\n","  2034,\n","  1012,\n","  18411,\n","  2860,\n","  2004,\n","  2521,\n","  2004,\n","  1996,\n","  1059,\n","  13876,\n","  1010,\n","  1045,\n","  2001,\n","  3241,\n","  3157,\n","  2008,\n","  2081,\n","  2039,\n","  2345,\n","  1998,\n","  1020,\n","  2005,\n","  1996,\n","  2694,\n","  2795,\n","  1010,\n","  1045,\n","  3191,\n","  2023,\n","  3720,\n","  2008,\n","  2360,\n","  1996,\n","  1059,\n","  13876,\n","  2345,\n","  2795,\n","  2003,\n","  2081,\n","  2039,\n","  1997,\n","  2184,\n","  2867,\n","  1010,\n","  2007,\n","  1996,\n","  2345,\n","  2416,\n","  2008,\n","  2191,\n","  2009,\n","  2006,\n","  2694,\n","  1010,\n","  1045,\n","  2074,\n","  2215,\n","  2000,\n","  2022,\n","  2469,\n","  2008,\n","  2027,\n","  2024,\n","  6149,\n","  1010,\n","  2043,\n","  1045,\n","  10651,\n","  1996,\n","  2867,\n","  18558,\n","  8758,\n","  26319,\n","  1010,\n","  4283,\n","  1618,\n","  100,\n","  1618,\n","  102],\n"," 'tgt': False,\n"," 'convo_id': 412567,\n"," 'comment_id': 2}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ppVvsaTYPuP","executionInfo":{"status":"ok","timestamp":1713386354234,"user_tz":240,"elapsed":12589,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"cb6c79a3-4cb3-4786-fb7d-b3750089f906"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def data_for_gpt2(data):\n","  src_text = data['src']\n","  reply_tokens = data['reply']\n","  tgt = data['tgt']\n","  decoded_reply = tokenizer_bert.decode(reply_tokens, skip_special_tokens=True)\n","  combined_text = f\"<SRC>: {src_text}  <REPLY>: {decoded_reply} <|endofcontext|>\"\n","  return combined_text, tgt"],"metadata":{"id":"jMz5_uwvT6p1","executionInfo":{"status":"ok","timestamp":1713386354234,"user_tz":240,"elapsed":4,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_data_gpt2 = []\n","tgt_train = []\n","for item in train_data:\n","  formatted_text, tgt = data_for_gpt2(item)\n","  train_data_gpt2.append(formatted_text)\n","  tgt_train.append(tgt)"],"metadata":{"id":"MxWoQ-9vYqah","executionInfo":{"status":"ok","timestamp":1713386367909,"user_tz":240,"elapsed":13678,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_data_gpt2[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"U4Sa5kNVY7sa","executionInfo":{"status":"ok","timestamp":1713386367909,"user_tz":240,"elapsed":26,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"d30b5d04-bee9-49ad-abd7-24fb099862ca"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<SRC>:  i notice that earier that moved wiki _ link to bill chen citing wiki _ link, then you reverted this change, bill chen doesn\\'t commonly go by william, his book is even penned as bill chen. from what i read in wp : commonname patrikr seems to be correct, examples given are names such as : * wiki _ link ( not wiki _ link ) * wiki _ link ( not wiki _ link ) i think this revert may have been a mistake unless you know otherwise? ▪ [UNK] ▪ [SEP]  chen was known in the poker world as \" william \" for years before he became commonly known as \" bill \". i changed it back because incidences online including usenet are roughly equal, nothing at all like bill clinton and william clinton, and in equal cases using the real name seems the best choice. ( the external _ link page is especially pschizo... willam in the page title, bill in the page text ). however i suppose the book is the trump card, so using the name on the book is probably best.  <REPLY>: i see what you saying i just read his pokerstars profile, it struck me when i saw the change because i remember him being called bill when i watched the last season of high stakes poker, but you seem to have many more years experience in the poker / gambling world then i do ( i\\'m still a bit of a newbie ), so i wanted to check with you first. btw as far as the wpt, i was thinking nine that made up final and 6 for the tv table, i read this article that say the wpt final table is made up of 10 players, with the final six that make it on tv, i just want to be sure that they are correct, when i update the players infobox stats, thanks ▪ ▪ <|endofcontext|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["tgt_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFwnIfGDZGnb","executionInfo":{"status":"ok","timestamp":1713386367910,"user_tz":240,"elapsed":9,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"048c5b42-25cc-48fa-843d-45d5eaadf690"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["val_data_gpt2 = []\n","tgt_val = []\n","for item in val_data:\n","  formatted_text, tgt = data_for_gpt2(item)\n","  val_data_gpt2.append(formatted_text)\n","  tgt_val.append(tgt)"],"metadata":{"id":"ntzdmk_0ZTOJ","executionInfo":{"status":"ok","timestamp":1713386369991,"user_tz":240,"elapsed":2088,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["val_data_gpt2[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"frCJvfndZfOb","executionInfo":{"status":"ok","timestamp":1713386369992,"user_tz":240,"elapsed":11,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"4a02737a-2aed-4af1-b83c-ada74cb7e8c6"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<SRC>:  you have been blocked for three hours for incivility. we cannot tolerate these types of edits : [SEP]  on the contrary, the edits you should not tolerate are the other side preaching their incorrect disambiguation. - - wiki _ link )  <REPLY>: i have unblocked you to allow you to reply to your rfar. <|endofcontext|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["tgt_val[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZNBUp2_Zhvw","executionInfo":{"status":"ok","timestamp":1713386369992,"user_tz":240,"elapsed":9,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"c2eed990-fe40-40de-e8af-5065035cb941"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["test_data_gpt2 = []\n","tgt_test = []\n","for item in val_data:\n","  formatted_text, tgt = data_for_gpt2(item)\n","  test_data_gpt2.append(formatted_text)\n","  tgt_test.append(tgt)"],"metadata":{"id":"mlERRSZHkVin","executionInfo":{"status":"ok","timestamp":1713386372056,"user_tz":240,"elapsed":2071,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"a6LN45CJ2fmV","executionInfo":{"status":"ok","timestamp":1713386372057,"user_tz":240,"elapsed":5,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"outputs":[],"source":["from transformers import AutoTokenizer\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from tqdm import tqdm"]},{"cell_type":"code","source":["from transformers import GPT2ForSequenceClassification, GPT2Config\n","import torch\n","import torch.nn.functional as F\n","\n","class GPT2ForSequenceClassificationMeanPooling(GPT2ForSequenceClassification):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.gpt2 = self.transformer  # Assuming the GPT2ForSequenceClassification model has a 'transformer' attribute\n","        self.score = torch.nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        labels=None,\n","        **kwargs\n","    ):\n","        outputs = self.gpt2(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            **kwargs,\n","        )\n","\n","        # Get the hidden states from the last layer\n","        hidden_states = outputs.last_hidden_state\n","\n","        # Apply mean pooling on the hidden states\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","        sum_hidden_states = torch.sum(hidden_states * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        mean_hidden_states = sum_hidden_states / sum_mask\n","\n","        # Pass the pooled hidden states through the classification head\n","        logits = self.score(mean_hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    loss_fct = torch.nn.MSELoss()\n","                    loss = loss_fct(logits.view(-1), labels.view(-1))\n","                else:\n","                    loss_fct = torch.nn.CrossEntropyLoss()\n","                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            else:\n","                raise ValueError(\"You should supply an instance of `PreTrainedModel` or a `config`\")\n","\n","        output = (logits,) + outputs[2:]\n","        return ((loss,) + output) if loss is not None else output\n"],"metadata":{"id":"sUFbzaGHWhf8","executionInfo":{"status":"ok","timestamp":1713386372356,"user_tz":240,"elapsed":303,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fK4brvRaCiC","executionInfo":{"status":"ok","timestamp":1713386372357,"user_tz":240,"elapsed":6,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"e9917144-ff18-42e8-96a8-e4ce2f14065b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Sf_rSjs166iN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713386375715,"user_tz":240,"elapsed":3361,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"95a6d94e-b9df-4c7b-9368-254ae18ae9f5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassificationMeanPooling were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.bias', 'score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["GPT2ForSequenceClassificationMeanPooling(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (score): Linear(in_features=768, out_features=2, bias=True)\n","  (gpt2): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n",")\n"]},{"output_type":"execute_result","data":{"text/plain":["GPT2ForSequenceClassificationMeanPooling(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (score): Linear(in_features=768, out_features=2, bias=True)\n","  (gpt2): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":18}],"source":["### Loading Model and Tokenizers\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2ForSequenceClassificationMeanPooling.from_pretrained('gpt2', num_labels=2)\n","print(model)\n","model.to(device)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"1IZZcTjs8E1F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713386376728,"user_tz":240,"elapsed":1017,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"a8d0bb28-ce59-4468-af25-197cfcf512f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["5\n"]},{"output_type":"execute_result","data":{"text/plain":["Embedding(50262, 768)"]},"metadata":{},"execution_count":19}],"source":["## Defining Special Tokens\n","\n","special_tokens = {'additional_special_tokens': ['<SRC>:', ' <REPLY>:', ' <|endofcontext|>','[UNK]','[SEP]']}\n","num_added = tokenizer.add_special_tokens(special_tokens)\n","print(num_added)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","source":["# Tokenizing Data\n","tokenizer.pad_token = tokenizer.eos_token\n","train_encodings = tokenizer(train_data_gpt2, truncation=False, padding=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3iySn85iDe4","executionInfo":{"status":"ok","timestamp":1713386385594,"user_tz":240,"elapsed":8869,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"3ed30c33-148a-46d8-9a55-89f2dd7276f8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1110 > 1024). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["max_length = max(len(x) for x in train_encodings.input_ids)\n"],"metadata":{"id":"p62VuuM_iTDg","executionInfo":{"status":"ok","timestamp":1713386385594,"user_tz":240,"elapsed":20,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["max_length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZurGT8HjE9v","executionInfo":{"status":"ok","timestamp":1713386385594,"user_tz":240,"elapsed":18,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"cb37fae1-ff58-49df-fb4f-d47bf8a20c5b"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1110"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["val_encodings = tokenizer(val_data_gpt2, truncation=False, padding=False)"],"metadata":{"id":"sheJKI37kvuQ","executionInfo":{"status":"ok","timestamp":1713386390523,"user_tz":240,"elapsed":4946,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["val_max_length = max(len(x) for x in val_encodings.input_ids)\n","val_max_length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdauY0M3k0gS","executionInfo":{"status":"ok","timestamp":1713386390523,"user_tz":240,"elapsed":18,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"085f86fa-69c0-4b8f-b5f3-f63daf10ce28"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1072"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["test_encodings = tokenizer(test_data_gpt2, truncation=False, padding=False)"],"metadata":{"id":"bfHhLMiGlU86","executionInfo":{"status":"ok","timestamp":1713386392932,"user_tz":240,"elapsed":2426,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["test_max_length = max(len(x) for x in test_encodings.input_ids)\n","test_max_length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OctYDORplYss","executionInfo":{"status":"ok","timestamp":1713386392933,"user_tz":240,"elapsed":5,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"1e4ba0cc-3636-4d4c-9e76-ad9b898ab09e"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1072"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["## Redefining Tokenizer with padding and max length\n","tokenizer.pad_token = tokenizer.eos_token\n","train_inputs = tokenizer(train_data_gpt2, truncation=True, padding=True, max_length=1024, return_tensors='pt')\n","\n","# Create torch dataset\n","input_ids = train_inputs['input_ids']\n","attention_mask = train_inputs['attention_mask']\n","labels = torch.tensor([0 if tgt == False else 1 for tgt in tgt_train])\n","train_dataset = TensorDataset(input_ids, attention_mask, labels)"],"metadata":{"id":"JBBJz_K6jih1","executionInfo":{"status":"ok","timestamp":1713386401811,"user_tz":240,"elapsed":8882,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["len(input_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TS3fPo7tHg3","executionInfo":{"status":"ok","timestamp":1713386401812,"user_tz":240,"elapsed":21,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"194cf638-48e4-4a5d-9266-566f966d6181"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1024"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["val_inputs = tokenizer(val_data_gpt2, truncation=True, padding=True, max_length=1024, return_tensors='pt')\n","val_input_ids = val_inputs['input_ids']\n","val_attention_mask = val_inputs['attention_mask']\n","val_labels = torch.tensor([0 if tgt == False else 1 for tgt in tgt_val])\n","val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)"],"metadata":{"id":"aOZCt_nHk7Sp","executionInfo":{"status":"ok","timestamp":1713386407385,"user_tz":240,"elapsed":5592,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["test_inputs = tokenizer(test_data_gpt2, truncation=True, padding=True, max_length=1024, return_tensors='pt')\n","test_input_ids = test_inputs['input_ids']\n","test_attention_mask = test_inputs['attention_mask']\n","test_labels = torch.tensor([0 if tgt == False else 1 for tgt in tgt_test])\n","test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)"],"metadata":{"id":"6yg_qNFQlhOQ","executionInfo":{"status":"ok","timestamp":1713386411667,"user_tz":240,"elapsed":4303,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["## CREATING DATALOADERS\n","batch_size = 2\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"9nRKJlHylF5J","executionInfo":{"status":"ok","timestamp":1713386411668,"user_tz":240,"elapsed":20,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from torch.optim import AdamW"],"metadata":{"id":"xYF9eT1jm2Sq","executionInfo":{"status":"ok","timestamp":1713386411668,"user_tz":240,"elapsed":18,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=1e-5)"],"metadata":{"id":"HRkoCPK3m5Tf","executionInfo":{"status":"ok","timestamp":1713386413261,"user_tz":240,"elapsed":1611,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["import copy"],"metadata":{"id":"7b0ja0mLntPQ","executionInfo":{"status":"ok","timestamp":1713386413261,"user_tz":240,"elapsed":4,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","\n","#Training and Validation Loop\n","model.train()\n","num_epochs = 1\n","train_losses = []\n","val_losses = []\n","f1_scores = []\n","best_val_loss = float('inf')\n","\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1} of {num_epochs}\")\n","    train_loss = 0\n","\n","    for batch in tqdm(train_dataloader, desc='Training Progress', miniters=10):\n","        input_ids, attention_mask, batch_labels = batch\n","        input_ids, attention_mask, batch_labels = input_ids.to(device), attention_mask.to(device), batch_labels.to(device)\n","        optimizer.zero_grad()\n","        model.config.pad_token_id = tokenizer.eos_token_id\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=batch_labels)\n","        loss = outputs[0]\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_train_loss = train_loss / len(train_dataloader)\n","    train_losses.append(avg_train_loss)\n","    print(f\"Training loss: {avg_train_loss:.2f}\")\n","\n","    val_loss = 0\n","    model.eval()\n","    val_preds = []\n","    val_true_labels = []\n","    with torch.no_grad():\n","      for batch in tqdm(val_dataloader, desc='Validation Progress', miniters = 10):\n","        input_ids, attention_mask, batch_labels = batch\n","        input_ids, attention_mask, batch_labels = input_ids.to(device), attention_mask.to(device), batch_labels.to(device)\n","        model.config.pad_token_id = tokenizer.eos_token_id\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=batch_labels)\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","\n","        logits = outputs.logits\n","        probs = torch.nn.functional.softmax(logits, dim=1)\n","        preds = torch.argmax(probs, dim=1)\n","        val_preds.extend(preds.cpu().numpy())\n","        val_true_labels.extend(batch_labels.cpu().numpy())\n","\n","      avg_val_loss = val_loss / len(val_dataloader)\n","      val_losses.append(avg_val_loss)\n","      f1 = f1_score(val_true_labels, val_preds, average='macro')\n","      f1_scores.append(f1)\n","      print(f\"Validation loss: {avg_val_loss:.2f}, F1 score: {f1:.2f}\")\n","      # print(f\"Confusion Matrix: {confusion_matrix(val_true_labels, val_preds)}\")\n","      # print(f\"Classification Report: {classification_report(val_true_labels, val_preds)}\")\n","      model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"TITrI5Aunwl4","executionInfo":{"status":"error","timestamp":1713386494860,"user_tz":240,"elapsed":81602,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}},"outputId":"43256f56-5015-4910-bdab-ae279b9c17f0"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 of 1\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:   0%|          | 1/1254 [01:20<27:56:20, 80.27s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-73456b5c24ad>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report"],"metadata":{"id":"rg_oZ84s5fmH","executionInfo":{"status":"aborted","timestamp":1713386494862,"user_tz":240,"elapsed":8,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Confusion Matrix:\\n {confusion_matrix(val_true_labels, val_preds)}\")\n","print(f\"Classification Report:\\n {classification_report(val_true_labels, val_preds)}\")"],"metadata":{"id":"D_5TxhfS5bXF","executionInfo":{"status":"aborted","timestamp":1713386494862,"user_tz":240,"elapsed":8,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jt9P6sPNV92i","executionInfo":{"status":"aborted","timestamp":1713386494862,"user_tz":240,"elapsed":8,"user":{"displayName":"Kshitiz Pokhrel","userId":"17765272327930284292"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyPocpE4CGnHSL3X9Tcn6Em7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}